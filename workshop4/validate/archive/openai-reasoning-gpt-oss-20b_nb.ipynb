{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "babaeb90",
   "metadata": {},
   "source": [
    "# ðŸš€ End-to-End Model Customization | Fine-tuning, Evaluation & Deployment\n",
    "\n",
    "This notebook demonstrates serverless training, evaluation pipelines, and model deployment for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694ac16-3b64-4906-8321-9694175e4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker --quiet  # restart the kernel after running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-setup",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup SageMaker session\n",
    "import boto3\n",
    "import os\n",
    "from rich import print as rprint\n",
    "from rich.pretty import pprint\n",
    "from sagemaker.core.helper.session_helper import Session\n",
    "\n",
    "REGION = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=REGION)\n",
    "\n",
    "# Create SageMaker session\n",
    "sagemaker_session = Session(sagemaker_client=sm_client)\n",
    "\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "\n",
    "# For MLFlow native metrics in Trainer wait, run below line with appropriate region\n",
    "os.environ[\"SAGEMAKER_MLFLOW_CUSTOM_ENDPOINT\"] = f\"https://mlflow.sagemaker.{REGION}.app.aws\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c995813",
   "metadata": {},
   "source": [
    "#### Create Training Dataset\n",
    "Below section provides sample code to create the training dataset arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.ai_registry.dataset import DataSet\n",
    "from sagemaker.ai_registry.dataset_utils import CustomizationTechnique\n",
    "import boto3\n",
    "\n",
    "# Register dataset in SageMaker AI Registry. This creates a versioned dataset that can be referenced by ARN\n",
    "dataset = DataSet.create(\n",
    "    name=\"demo-sft-dataset\",\n",
    "    source=\"s3://your-bucket/dataset/training_dataset.jsonl\", # Source can be S3 or local path\n",
    "    #customization_technique=CUSTOMIZATION_TECHNIQUE.SFT # or DPO or RLVR\n",
    "        #Optional technique name for minimal dataset format check.\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"TRAINING_DATASET ARN: {dataset.arn}\")\n",
    "# TRAINING_DATASET = dataset.arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1f611-45c4-4cc2-9eb3-f24be5891ca4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required Configs\n",
    "BASE_MODEL = \"\"\n",
    "\n",
    "# MODEL_PACKAGE_GROUP_NAME is same as CUSTOM_MODEL_NAME\n",
    "MODEL_PACKAGE_GROUP_NAME = \"\"\n",
    "\n",
    "TRAINING_DATASET = \"\"\n",
    "\n",
    "S3_OUTPUT_PATH = \"\"\n",
    "\n",
    "ROLE_ARN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056d1256a310f44",
   "metadata": {},
   "source": [
    "#### Create Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd1e05bce76fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import ModelPackageGroup\n",
    "model_package_group = ModelPackageGroup.create(\n",
    "    model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
    "    model_package_group_description='' #Required Description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c533f18",
   "metadata": {},
   "source": [
    "#### End-user license agreements (EULA)\n",
    "Some foundation models require explicit acceptance of an end-user license agreement (EULA) before use. Please explicitly change the below variable `ACCEPT_EULA` to `True` to agree to the terms and conditions. For more details please refer https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-choose.html/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7cf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPT_EULA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finetuning-section",
   "metadata": {},
   "source": [
    "# Part 1: Fine-tuning\n",
    "\n",
    "### Step 1: Creating the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0718631",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": ""
   },
   "source": [
    "#### Choose one of the following trainer techniques:\n",
    "- **Option 1: SFT Trainer (Supervised Fine-Tuning)** \n",
    "- **Option 2: Create RLVRTrainer (Reinforcement Learning with Verifiable Rewards)**. \n",
    "- **Option 3: RLAIF Trainer (Reinforcement Learning from AI Feedback)** \n",
    "- **Option 4: DPO Trainer (Direct Preference Optimization)** \n",
    "\n",
    "**Instructions:** Run only ONE of the trainers, not all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-create-trainer",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "SFT"
   },
   "source": [
    "#### Create SFT Trainer (Supervised Fine-Tuning)\n",
    "\n",
    "##### Key Parameters:\n",
    "* `model`: base_model id on Sagemaker Hubcontent that is available to finetune (or) ModelPackage artifacts\n",
    "* `training_type`: Choose from TrainingType Enum(sagemaker.train.common) either LORA OR FULL. (optional)\n",
    "* `model_package_group`: ModelPackage group name or ModelPackageGroup (optional)\n",
    "* `mlflow_resource_arn`: MLFlow app ARN to track the training job (optional)\n",
    "* `mlflow_experiment_name`: MLFlow app experiment name(str) (optional)\n",
    "* `mlflow_run_name`: MLFlow app run name(str) (optional)\n",
    "* `training_dataset`: Training Dataset - either Dataset ARN or S3 Path of the dataset (Please note these are required for a training job to run, can be either provided via Trainer or .train()) (optional)\n",
    "* `validation_dataset`: Validation Dataset - either Dataset ARN or S3 Path of the dataset (optional)\n",
    "* `s3_output_path`: S3 path for the trained model artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sft-trainer",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "SFT"
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.sft_trainer import SFTTrainer\n",
    "from sagemaker.train.common import TrainingType\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=BASE_MODEL,\n",
    "    training_type=TrainingType.LORA,\n",
    "    model_package_group=model_package_group,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    s3_output_path=S3_OUTPUT_PATH,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept_eula=ACCEPT_EULA,\n",
    "    role=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055c93b",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": ""
   },
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e416137-1e40-48c9-90aa-14a748885e1f",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "RLVR"
   },
   "source": [
    "#### Create RLVRTrainer (Reinforcement Learning with Verifiable Rewards)\n",
    "\n",
    "##### Key Parameters:\n",
    "* `model`: base_model id on Sagemaker Hubcontent that is available to finetune (or) ModelPackage artifacts\n",
    "* `custom_reward_function`: Custom reward function/Evaluator ARN (optional)\n",
    "* `model_package_group`: ModelPackage group name or ModelPackageGroup (optional)\n",
    "* `mlflow_resource_arn`: MLFlow app ARN to track the training job (optional)\n",
    "* `mlflow_experiment_name`: MLFlow app experiment name(str) (optional)\n",
    "* `mlflow_run_name`: MLFlow app run name(str) (optional)\n",
    "* `training_dataset`: Training Dataset - either Dataset ARN or S3 Path of the dataset (Please note these are required for a training job to run, can be either provided via Trainer or .train()) (optional)\n",
    "* `validation_dataset`: Validation Dataset - either Dataset ARN or S3 Path of the dataset (optional)\n",
    "* `s3_output_path`: S3 path for the trained model artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31544c05-6fde-420c-99cb-eb1b3ba4734f",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "RLVR"
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.rlvr_trainer import RLVRTrainer\n",
    "\n",
    "\n",
    "trainer = RLVRTrainer(\n",
    "    model=BASE_MODEL,\n",
    "    model_package_group=model_package_group,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    s3_output_path=S3_OUTPUT_PATH,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept_eula=ACCEPT_EULA,\n",
    "    role=ROLE_ARN\n",
    ")\n",
    "\n",
    "# You can pass the custom reward function to the trainer.\n",
    "# CUSTOM_REWARD_FUNCTION = \"arn:aws:sagemaker:<region>:<accountId>:hub-content/<HUB-NAME>/JsonDoc/<CUSTOM_REWARD_FUNCTION>/<VERSION>\"\n",
    "#\n",
    "# trainer = RLVRTrainer(\n",
    "#     model=BASE_MODEL,\n",
    "#     model_package_group=model_package_group,\n",
    "#     training_dataset=TRAINING_DATASET,\n",
    "#     custom_reward_function = CUSTOM_REWARD_FUNCTION\n",
    "#     s3_output_path=S3_OUTPUT_PATH,\n",
    "#     sagemaker_session=sagemaker_session,\n",
    "#     accept_eula=ACCEPT_EULA,\n",
    "#     role=ROLE_ARN\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800d46a",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": ""
   },
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a57ebc-9d94-4c85-8b4d-c6bcead0b896",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "RLAIF"
   },
   "source": [
    "#### Create RLAIF Trainer (Reinforcement Learning from AI Feedback)\n",
    "This trainer uses AI models as reward functions\n",
    "\n",
    "##### Key Parameters:\n",
    "* `model`: base_model id on Sagemaker Hubcontent that is available to finetune (or) ModelPackage artifacts\n",
    "* `reward_model_id`: Bedrock model id, supported evaluation models: https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-judge.html (optional)\n",
    "*  `reward_prompt`: Reward prompt ARN or builtin prompts refer: https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-metrics.html (optional)\n",
    "* `model_package_group`: ModelPackage group name or ModelPackageGroup (optional)\n",
    "* `mlflow_resource_arn`: MLFlow app ARN to track the training job (optional)\n",
    "* `mlflow_experiment_name`: MLFlow app experiment name(str) (optional)\n",
    "* `mlflow_run_name`: MLFlow app run name(str) (optional)\n",
    "* `training_dataset`: Training Dataset - either Dataset ARN or S3 Path of the dataset (Please note these are required for a training job to run, can be either provided via Trainer or .train()) (optional)\n",
    "* `validation_dataset`: Validation Dataset - either Dataset ARN or S3 Path of the dataset (optional)\n",
    "* `s3_output_path`: S3 path for the trained model artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlaif-trainer-setup",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "RLAIF"
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.rlaif_trainer import RLAIFTrainer\n",
    "\n",
    "# example values for REWARD MODEL and PROMPT\n",
    "REWARD_MODEL_ID = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "REWARD_PROMPT = 'Builtin.Correctness'\n",
    "\n",
    "\n",
    "trainer = RLAIFTrainer(\n",
    "    model=BASE_MODEL,\n",
    "    model_package_group=model_package_group,\n",
    "    reward_model_id=REWARD_MODEL_ID,\n",
    "    reward_prompt=REWARD_PROMPT,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    s3_output_path=S3_OUTPUT_PATH,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept_eula=ACCEPT_EULA,\n",
    "    role=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0dbdad",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": ""
   },
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ccf01",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "DPO"
   },
   "source": [
    "#### Create DPO Trainer (Direct Preference Optimization)\n",
    "\n",
    "Direct Preference Optimization (DPO) is a method for training language models to follow human preferences. Unlike traditional RLHF (Reinforcement Learning from Human Feedback), DPO directly optimizes the model using preference pairs without needing a reward model.\n",
    "\n",
    "##### Key Parameters:\n",
    "- `model` Base model to fine-tune (from SageMaker Hub)\n",
    "- `training_type` Fine-tuning method (LoRA recommended for efficiency)\n",
    "- `training_dataset` ARN of the registered preference dataset\n",
    "- `model_package_group` Where to store the fine-tuned model\n",
    "- `mlflow_resource_arn` MLflow tracking server for experiment logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dpo-trainer-setup",
   "metadata": {
    "editable": true,
    "jumpStartAlterations": [
     "trainerSelection"
    ],
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "trainer_type": "DPO"
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.dpo_trainer import DPOTrainer\n",
    "from sagemaker.train.common import TrainingType\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=BASE_MODEL,\n",
    "    training_type=TrainingType.LORA,\n",
    "    model_package_group=model_package_group,\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    s3_output_path=S3_OUTPUT_PATH,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept_eula=ACCEPT_EULA,\n",
    "    role=ROLE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588635b-a404-4623-80bd-a5d103799f15",
   "metadata": {},
   "source": [
    "### Step 2: Get Finetuning Options and Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlaif-options-info",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Default Finetuning Options:\")\n",
    "pprint(trainer.hyperparameters.to_dict())\n",
    "\n",
    "# Modify options like object attributes\n",
    "trainer.hyperparameters.learning_rate = 0.00001\n",
    "\n",
    "print(\"\\nModified/User defined Options:\")\n",
    "pprint(trainer.hyperparameters.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f8994-f94f-405a-927d-fd60596d0850",
   "metadata": {},
   "source": [
    "### Step 3: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlaif-training",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job = trainer.train(wait=True)\n",
    "\n",
    "TRAINING_JOB_NAME = training_job.training_job_name\n",
    "\n",
    "pprint(training_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfa457-72f2-4861-9d08-49c21061b30b",
   "metadata": {},
   "source": [
    "### Step 4: Describe Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8b553-5202-4c2a-b8a7-8cd8be72259e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import TrainingJob\n",
    "\n",
    "response = TrainingJob.get(training_job_name=TRAINING_JOB_NAME)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-section",
   "metadata": {},
   "source": [
    "# Part 2: Model Evaluation\n",
    "\n",
    "This section demonstrates the basic user-facing flow for creating and managing evaluation jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7af03e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 1: Choose one of the following evaluation techniques:\n",
    "- **Option 1: BenchmarkEvaluator** \n",
    "- **Option 2: LLMAsJudgeEvaluator (LLM-as-Judge Evaluation)** \n",
    "- **Option 3: CustomScorerEvaluator** \n",
    "\n",
    "**Instructions:** Run only ONE of the technique sections, not all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-pipeline",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Option 1: Create BenchmarkEvaluator\n",
    "\n",
    "Create a BenchmarkEvaluator instance with the desired benchmark. The evaluator will use Jinja2 templates to render a complete pipeline definition.\n",
    "\n",
    "### Key Parameters:\n",
    "- `benchmark`: Benchmark type from the Benchmark enum\n",
    "- `model`: Model ARN from SageMaker hub content\n",
    "- `s3_output_path`: S3 location for evaluation outputs\n",
    "- `mlflow_resource_arn`: MLflow tracking server ARN for experiment tracking (optional)\n",
    "- `model_package_group`: Model package group ARN (optional)\n",
    "- `source_model_package`: Source model package ARN (optional)\n",
    "- `model_artifact`: ARN of model artifact for lineage tracking (auto-inferred from source_model_package) (optional)\n",
    "\n",
    "**Note:** When you call `evaluate()`, the system will start evaluation job. The evaluator will:\n",
    "1. Build template context with all required parameters\n",
    "2. Render the pipeline definition from `DETERMINISTIC_TEMPLATE` using Jinja2\n",
    "3. Create or update the pipeline with the rendered definition\n",
    "4. Start the pipeline execution with empty parameters (all values pre-substituted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete-existing-pipeline",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.evaluate import BenchMarkEvaluator\n",
    "from sagemaker.train.evaluate import get_benchmarks, get_benchmark_properties\n",
    "from rich.pretty import pprint\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(name)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Get available benchmarks\n",
    "Benchmark = get_benchmarks()\n",
    "pprint(list(Benchmark))\n",
    "\n",
    "# Print properties for a specific benchmark\n",
    "pprint(get_benchmark_properties(benchmark=Benchmark.MMLU))\n",
    "\n",
    "\n",
    "# Create evaluator with GEN_QA benchmark\n",
    "evaluator = BenchMarkEvaluator(\n",
    "    benchmark=Benchmark.MMLU,\n",
    "    model=BASE_MODEL,\n",
    "    s3_output_path=S3_OUTPUT_PATH,\n",
    ")\n",
    "\n",
    "pprint(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e025e9e-8ce7-407b-b9dd-0ac7ddb2d66d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Option 2: Create LLMAsJudgeEvaluator\n",
    "\n",
    "Create an LLMAsJudgeEvaluator instance with the desired evaluator model, dataset, and metrics.\n",
    "\n",
    "### Key Parameters:\n",
    "- `model`: Model ARN to be evaluated (required)\n",
    "- `evaluator_model`: Bedrock model ID to use as judge (required)\n",
    "- `dataset`: S3 URI or Dataset ARN (required)\n",
    "- `s3_output_path`: S3 output location (required)\n",
    "- `builtin_metrics`: List of built-in metrics (optional, no 'Builtin.' prefix needed)\n",
    "- `custom_metrics`: JSON string of custom metrics (optional)\n",
    "- `evaluate_base_model`: Whether to evaluate base model in addition to custom model (optional, default=True)\n",
    "- `mlflow_resource_arn`: MLflow tracking server ARN (optional)\n",
    "- `model_package_group`: Model package group ARN (optional)\n",
    "- `source_model_package`: Source model package ARN (optional)\n",
    "\n",
    "#### Using custom metrics (as JSON string)\n",
    "\n",
    "Custom metrics must be provided as a properly escaped JSON string. You can either:\n",
    "1. Create a Python dict and use `json.dumps()` to convert it\n",
    "2. Provide a pre-escaped JSON string directly\n",
    "\n",
    "**Note:** When you call `evaluate()`, the system will start evaluation job. The evaluator will:\n",
    "1. Generate inference responses from the base model (if evaluate_base_model=True)\n",
    "2. Generate inference responses from the custom model\n",
    "3. Use the judge model to evaluate responses with built-in and custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478a194",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from rich.pretty import pprint\n",
    "from sagemaker.train.evaluate import LLMAsJudgeEvaluator\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(name)s - %(message)s'\n",
    ")\n",
    "\n",
    "EVALUATOR_MODEL = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "BUILTIN_METRICS=[\"Completeness\", \"Faithfulness\"]\n",
    "\n",
    "EVALUATION_DATASET = \"\"\n",
    "\n",
    "custom_metrics_list = [\n",
    "    {\n",
    "        \"customMetricDefinition\": {\n",
    "            \"name\": \"GoodMetric\",\n",
    "            \"instructions\": (\n",
    "                \"Assess if the response has positive sentiment. \"\n",
    "                \"Prompt: {{prompt}}\\nResponse: {{prediction}}\"\n",
    "            ),\n",
    "            \"ratingScale\": [\n",
    "                {\"definition\": \"Good\", \"value\": {\"floatValue\": 1}},\n",
    "                {\"definition\": \"Poor\", \"value\": {\"floatValue\": 0}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"customMetricDefinition\": {\n",
    "            \"name\": \"BadMetric\",\n",
    "            \"instructions\": (\n",
    "                \"Assess if the response has negative sentiment. \"\n",
    "                \"Prompt: {{prompt}}\\nResponse: {{prediction}}\"\n",
    "            ),\n",
    "            \"ratingScale\": [\n",
    "                {\"definition\": \"Bad\", \"value\": {\"floatValue\": 1}},\n",
    "                {\"definition\": \"Good\", \"value\": {\"floatValue\": 0}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_metrics_json = json.dumps(custom_metrics_list)\n",
    "\n",
    "# Alternate Option: Create metrics using dict\n",
    "#\n",
    "# custom_metric_dict = {\n",
    "#     \"customMetricDefinition\": {\n",
    "#         \"name\": \"PositiveSentiment\",\n",
    "#         \"instructions\": (\n",
    "#             \"You are an expert evaluator. Your task is to assess if the sentiment of the response is positive. \"\n",
    "#             \"Rate the response based on whether it conveys positive sentiment, helpfulness, and constructive tone.\\n\\n\"\n",
    "#             \"Consider the following:\\n\"\n",
    "#             \"- Does the response have a positive, encouraging tone?\\n\"\n",
    "#             \"- Is the response helpful and constructive?\\n\"\n",
    "#             \"- Does it avoid negative language or criticism?\\n\\n\"\n",
    "#             \"Rate on this scale:\\n\"\n",
    "#             \"- Good: Response has positive sentiment\\n\"\n",
    "#             \"- Poor: Response lacks positive sentiment\\n\\n\"\n",
    "#             \"Here is the actual task:\\n\"\n",
    "#             \"Prompt: {{prompt}}\\n\"\n",
    "#             \"Response: {{prediction}}\"\n",
    "#         ),\n",
    "#         \"ratingScale\": [\n",
    "#             {\"definition\": \"Good\", \"value\": {\"floatValue\": 1}},\n",
    "#             {\"definition\": \"Poor\", \"value\": {\"floatValue\": 0}}\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "#\n",
    "# Convert to JSON string\n",
    "# custom_metrics_json = json.dumps([custom_metric_dict])\n",
    "\n",
    "\n",
    "# Create evaluator with custom metrics\n",
    "evaluator = LLMAsJudgeEvaluator(\n",
    "    model=BASE_MODEL,  # Required\n",
    "    evaluator_model=EVALUATOR_MODEL,  # Required\n",
    "    dataset=EVALUATION_DATASET,  # Required: S3 URI or Dataset ARN\n",
    "    builtin_metrics=BUILTIN_METRICS,  # Optional: Can combine with custom metrics\n",
    "    custom_metrics=custom_metrics_json,  # Optional: JSON string of custom metrics\n",
    "    s3_output_path=S3_OUTPUT_PATH,  # Required\n",
    "    evaluate_base_model=False  # Skip base model evaluation to evaluate only custom model\n",
    ")\n",
    "\n",
    "pprint(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b2a17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Option 3: SageMaker Custom Scorer Evaluation\n",
    "\n",
    "Instantiate the evaluator with your configuration. The evaluator can accept:\n",
    "- **Custom Evaluator ARN (string):** Points to your custom evaluator in AI Registry\n",
    "- **Built-in Metric (string or enum):** Use preset metrics like \"code_executions\", \"math_answers\", etc.\n",
    "- **Evaluator Object:** A sagemaker.ai_registry.evaluator.Evaluator instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5bb6f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.train.evaluate import CustomScorerEvaluator\n",
    "from sagemaker.train.evaluate import get_builtin_metrics\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(name)s - %(message)s'\n",
    ")\n",
    "\n",
    "EVALUATION_DATASET = \"\"\n",
    "# Evaluator ARN (custom evaluator from AI Registry)\n",
    "EVALUATOR_ARN = \"\"\n",
    "\n",
    "# Create evaluator with evaluator arn\n",
    "evaluator = CustomScorerEvaluator(\n",
    "    evaluator=EVALUATOR_ARN,\n",
    "    model=BASE_MODEL,  # Required str: Model identifier from hub content\n",
    "    dataset=EVALUATION_DATASET,  # Required Any: Dataset for evaluation (S3 path or AIR dataset object)\n",
    "    s3_output_path=S3_OUTPUT_PATH  # Required str: S3 bucket URI for evaluation outputs\n",
    ")\n",
    "\n",
    "pprint(evaluator)\n",
    "\n",
    "\n",
    "# Alternate Option 1: Use built-in metrics\n",
    "\n",
    "# from sagemaker.train.evaluate import get_builtin_metrics\n",
    "# \n",
    "# BuiltInMetric = get_builtin_metrics()\n",
    "# \n",
    "# pprint(list(BuiltInMetric)) # Display available preset metrics\n",
    "# \n",
    "# evaluator_builtin = CustomScorerEvaluator(\n",
    "#     evaluator=BuiltInMetric.PRIME_MATH,  # Or use string: \"prime_math\"\n",
    "#     dataset=EVALUATION_DATASET,\n",
    "#     base_model=BASE_MODEL,\n",
    "#     s3_output_path=S3_OUTPUT_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation",
   "metadata": {},
   "source": [
    "## Step 2: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-evaluation",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "execution = evaluator.evaluate()\n",
    "\n",
    "print(f\"Evaluation job started!\")\n",
    "print(f\"Job ARN: {execution.arn}\")\n",
    "print(f\"Job Name: {execution.name}\")\n",
    "print(f\"Status: {execution.status.overall_status}\")\n",
    "\n",
    "pprint(execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de8255-9f98-444a-99a6-cfe7cc2584af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 3: Monitor Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor-evaluation",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution.refresh()\n",
    "\n",
    "print(f\"Current status: {execution.status}\")\n",
    "\n",
    "# Display individual step statuses\n",
    "if execution.status.step_details:\n",
    "    print(\"\\nStep Details:\")\n",
    "    for step in execution.status.step_details:\n",
    "        print(f\"  {step.name}: {step.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebac85a-adee-4f18-935d-478037c7a1f3",
   "metadata": {},
   "source": [
    "## Step 4: Wait for Completion\n",
    "\n",
    "Wait for the pipeline to complete. This provides rich progress updates in Jupyter notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b51cca-2024-4276-b05d-48f52e527c06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution.wait(target_status=\"Succeeded\", poll=5, timeout=3600)\n",
    "\n",
    "print(f\"\\nFinal Status: {execution.status.overall_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d153370-213a-41d0-8a95-f4ffccf8f9aa",
   "metadata": {},
   "source": [
    "## Step 5: View Results\n",
    "\n",
    "Display the evaluation results in a formatted table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f470824-7740-48bb-9282-a7b9d0407fff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "execution.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-section",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 3: End-to-End Model Deployment with SageMaker\n",
    "\n",
    "This comprehensive notebook demonstrates deployment workflows for fine-tuned large language models (LLMs) using Amazon SageMaker.\n",
    "\n",
    "Chose one of the below deployment options.\n",
    "\n",
    "**Option 1. Deploy using Model Builder**\n",
    "\n",
    "**Option 2. Deploy using Bedrock Model Builder**\n",
    "\n",
    "**Instructions:** Run only ONE of the options, not all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ae9e9-45d4-415e-8681-0c192bc951ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Option 1: Deploy using Model Builder:\n",
    "\n",
    "ModelBuilder has three ways of building the model.\n",
    "\n",
    "**Option A. Model Builder using TrainingJob**: Take a completed fine-tuning job and deploy it directly as a real-time inference endpoint.\n",
    "\n",
    "**Option B. Model Builder using ModelPackage**: Use versioned model packages from the SageMaker Model Registry for deployment\n",
    "\n",
    "**Option C. Model Builder using Trainer**: Use fine-tuning interfaces through resource chaining, so users do not have to manually pass in the model weights\n",
    "\n",
    "Above approaches support:\n",
    "- Standalone endpoint deployment (dedicated resources)\n",
    "- Multi-adapter deployment (shared base model with multiple fine-tuned adapters)\n",
    "\n",
    "\n",
    "**Instructions:** Run only ONE of the options, not all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2aa329-2be6-4fea-b760-c0cb0be42f78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Option A: Model Builder using TrainingJob\n",
    "\n",
    "This section demonstrates the most direct deployment path: taking a completed SageMaker training job and deploying it as a real-time inference endpoint. This approach is ideal when you've just finished fine-tuning a model and want to immediately deploy it for testing or production use. Use the SageMaker ModelBuilder to prepare the trained model for deployment. ModelBuilder performs several critical tasks:\n",
    "\n",
    "**Key Benefits:**\n",
    "- Direct deployment from training artifacts\n",
    "- No intermediate model registration required\n",
    "- Fastest path from training to inference\n",
    "- Automatic model artifact resolution\n",
    "\n",
    "\n",
    "**ModelBuilder will:**\n",
    "1. Validates the training job artifacts and metadata\n",
    "2. Creates a SageMaker Model resource with appropriate container configurations\n",
    "3. Sets up inference specifications (input/output handling)\n",
    "4. Configures model data location and IAM roles\n",
    "5. Prepares the model for either standalone or multi-adapter deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e08d2-0da3-455e-9b7e-0598fa86660e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker.serve import ModelBuilder\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "\n",
    "name = f\"e2e-{random.randint(100, 100000)}\"\n",
    "\n",
    "print(f\"Endpoint Name: {name}\")\n",
    "print(f\"Training Job Name: {TRAINING_JOB_NAME}\")\n",
    "\n",
    "training_job = TrainingJob.get(training_job_name=TRAINING_JOB_NAME)\n",
    "model_builder = ModelBuilder(model=training_job)\n",
    "model_builder.build(model_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc2cbd-6fd3-436a-8169-412148650e5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Option B: Model Builder using ModelPackage\n",
    "\n",
    "**When to Use ModelPackages:**\n",
    "- Production deployments requiring approval gates\n",
    "- Multi-environment deployments (dev, staging, prod)\n",
    "- Models shared across teams or accounts\n",
    "- Compliance and audit requirements\n",
    "- Deploy any approved version, not just the latest training run\n",
    "\n",
    "ModelPackages are automatically created when training jobs complete, or can be registered manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4522f-8865-4d3b-be7d-f00ee8f81d6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**ModelPackage Metadata:**\n",
    "- **Group**: 'test-finetuned-models' (collection of related model versions)\n",
    "- **Version**: 3 (specific iteration of the fine-tuned model)\n",
    "- **Status**: Completed (ready for deployment)\n",
    "\n",
    "**Inference Specification:**\n",
    "- Model artifacts location in S3\n",
    "- Base model reference\n",
    "- Recipe name for fine-tuning configuration\n",
    "- Container and runtime requirements\n",
    "\n",
    "**The key difference between ModelPackage vs TrainingJob Deployment:**\n",
    "- **ModelPackage**: Uses versioned, approved artifacts from Model Registry\n",
    "- **TrainingJob**: Uses artifacts directly from training output\n",
    "\n",
    "ModelBuilder automatically resolves all necessary metadata from the ModelPackage, including model artifacts, base model references, and inference configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01beec55-8024-4327-b461-280ff356c077",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.core.resources import ModelPackage\n",
    "import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "import random\n",
    "from sagemaker.serve import ModelBuilder\n",
    "\n",
    "MODEL_PACKAGE_ARN = \"\"\n",
    "\n",
    "model_package = ModelPackage.get(model_package_name=MODEL_PACKAGE_ARN)\n",
    "\n",
    "model_builder = ModelBuilder(model=model_package)\n",
    "model_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1669df0",
   "metadata": {},
   "source": [
    "### Option C: Model Builder using Trainer\n",
    "\n",
    "Model Builder also supports handshake with fine-tuning interfaces through resource chaining, so users do not have to manually pass in the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sagemaker.serve import ModelBuilder\n",
    "\n",
    "name = f\"e2e-{random.randint(100, 10000)}\"\n",
    "\n",
    "# Note: trainer is created in Part 1.\n",
    "model_builder = ModelBuilder(model=trainer)\n",
    "model_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc628f-f271-41ec-91d4-e1570191e59d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Deploy as Standalone Endpoint or Inference Component (Adapter)\n",
    "\n",
    "Two ways to deploy the model:\n",
    "- **Standalone Endpoint**: The endpoint will be created with the name specified in the `name` variable and will be ready to accept inference requests once deployment completes (typically 5-10 minutes).\n",
    "- **Deploy as Inference Component (Adapter)**: Deploy the fine-tuned model as an InferenceComponent (adapter) on an existing endpoint. The `inference_component_name` parameter identifies this specific adapter for routing requests.\n",
    "  - **Use Cases of Deploy as Inference Component:**\n",
    "      - Serving multiple fine-tuned variants (e.g., different domains, languages, or tasks)\n",
    "      - A/B testing different fine-tuning approaches\n",
    "      - Multi-tenant deployments with isolated adapters per customer\n",
    "      - Route requests to specific adapters via inference component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927027dd-35a7-458d-9c1c-ca3aace8f7f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy as Standalone Endpoint \n",
    "endpoint = model_builder.deploy(endpoint_name=name)\n",
    "\n",
    "# OR\n",
    "# Deploy as Inference Component (Adapter)\n",
    "# endpoint = model_builder.deploy(endpoint_name=name, inference_component_name=f\"{name}-adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3048fec-d9ee-4f08-a176-0bc41b86532a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Test the Endpoint\n",
    "\n",
    "Validate the deployed endpoint by sending a test inference request. This example demonstrates:\n",
    "\n",
    "**Request Format:**\n",
    "- **inputs**: The prompt text for the model\n",
    "- **parameters**: Inference configuration\n",
    "  - `max_new_tokens`: Maximum length of generated response (50 tokens)\n",
    "\n",
    "**Expected Behavior:**\n",
    "- The endpoint processes the prompt through the fine-tuned model\n",
    "- Returns generated text based on the model's training\n",
    "- Response includes the generated text and metadata\n",
    "\n",
    "This test confirms the endpoint is operational and the model is responding correctly to inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d66c68-9450-46c7-a9bb-16515c7697d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client(\n",
    "    'sagemaker-runtime',\n",
    "    region_name=REGION\n",
    ")\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=name,\n",
    "    Body=json.dumps({\"inputs\": \"What is the capital of France?\", \"parameters\": {\"max_new_tokens\": 50}}),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18627e85",
   "metadata": {},
   "source": [
    "## Option 2: Bedrock Model Builder\n",
    "\n",
    "Below section highlights the working flow for deploying the model using Bedrock Model Builder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.bedrock_model_builder import BedrockModelBuilder\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "import random\n",
    "\n",
    "training_job = TrainingJob.get(training_job_name=TRAINING_JOB_NAME)\n",
    "name = f\"e2e-{random.randint(100, 100000)}\"\n",
    "\n",
    "bedrock_builder = BedrockModelBuilder(model=training_job)\n",
    "\n",
    "#deploy the model\n",
    "bedrock_builder.deploy(job_name=name, imported_model_name=name, role_arn=ROLE_ARN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
